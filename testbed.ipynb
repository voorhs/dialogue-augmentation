{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 615/615 [00:00<00:00, 80.3kB/s]\n",
      "Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 21.4MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 9.10M/9.10M [00:00<00:00, 34.0MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250002"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "len(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 300/300 [00:00<00:00, 45.1kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 670/670 [00:00<00:00, 282kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 22.3MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 50.5kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained('Shitao/RetroMAE_MSMARCO')\n",
    "len(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'500.json'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "chunk_names = [fname for fname in os.listdir('data/augmented/back-translate/') if fname.endswith('.json')]\n",
    "chunk_names[414]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyarrow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\n",
    "    {\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"Hey.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"Hey, what can I do for you?\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"I'd like to visit Chicago, Il-Ord, to meet my best friend, so I need to book a plane ticket.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"Okay.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"Could you book me a ticket for 09/09?\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"Please send me the place of origin.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"My name is Stephanie Carter.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"May I ask the date of arrival, please?\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"I'd like to arrive at 09/11.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"Please send the exact location of your origin.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"Of course, my origins are Dallas and Fort Worth, TX - DFW.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"Do you have any other specifications?\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"Yeah, I need to stop between my journey.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"I found a Hawaiian airline number 1003, which meets your criteria.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"All right, please book the tickets.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"Your ticket is booked.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"Thank you.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"Thank you.\"\n",
    "            }\n",
    "        ],\n",
    "        \"source_dataset_name\": \"AirDialogue\",\n",
    "        \"idx_within_source\": 187590,\n",
    "        \"idx\": 0\n",
    "    },\n",
    "    {\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"Hey.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"Hey, what can I do for you today?\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"I'm Anthony Martinez from Charlotte, I want to go to Orlando this month to attend the summit, could you book a ticket from the CLT to MCO?\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"I'm here to help you with that.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"Thank you, I want to leave for 08/13 and book my return ticket for 08/15, too.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"Wait a minute.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"Okay.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"There's a one-stop flight with a 100 ticket rate, which is provided by Jet Blue airlines.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"Please continue.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"I booked your flight 1026.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 0,\n",
    "                \"utterance\": \"Thank you for your help.\"\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": 1,\n",
    "                \"utterance\": \"Greetings, thank you for choosing our service.\"\n",
    "            }\n",
    "        ],\n",
    "        \"source_dataset_name\": \"AirDialogue\",\n",
    "        \"idx_within_source\": 285622,\n",
    "        \"idx\": 1\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    'MS-DC',\n",
    "    'MetaLWOZ',\n",
    "    'MULTIWOZ2_2',\n",
    "    'SGD',\n",
    "    'SimJointGEN',\n",
    "    'KETOD',\n",
    "    'FRAMES',\n",
    "    'Disambiguation',\n",
    "    'ABCD',\n",
    "    'AirDialogue',\n",
    "    'BiTOD',\n",
    "    'Taskmaster1'\n",
    "]\n",
    "\n",
    "upper_bound = 96\n",
    "\n",
    "upper_bounds = {\n",
    "    'MS-DC': min(upper_bound, 250),\n",
    "    'MetaLWOZ': min(upper_bound, 100),\n",
    "    'MULTIWOZ2_2': min(upper_bound, 75),\n",
    "    'SGD': None,\n",
    "    'SimJointGEN': upper_bound,\n",
    "    'KETOD': upper_bound,\n",
    "    'FRAMES': upper_bound,\n",
    "    'Disambiguation': min(upper_bound, 60),\n",
    "    'ABCD': upper_bound,\n",
    "    'AirDialogue': upper_bound,\n",
    "    'BiTOD': upper_bound,\n",
    "    'Taskmaster1': min(upper_bound, 200),\n",
    "}\n",
    "\n",
    "\n",
    "from random import shuffle, seed as set_seet\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from functools import partial\n",
    "from typing import Tuple, List\n",
    "from tqdm import tqdm\n",
    "from mylib.utils.data import Dialogue, ContextResponsePair\n",
    "import json\n",
    "import os\n",
    "import pyarrow as pa\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def preprocess_dialogue(\n",
    "        raw_sample,\n",
    "        tokenizer,\n",
    "        bound=None,\n",
    "        user_id=0,\n",
    "        system_id=1,\n",
    "    ):\n",
    "    \"\"\"convert single dialogue (in DialogStudio format) to list of utterances and list of corresponding speaker ids\"\"\"\n",
    "    \n",
    "    if is_empty(raw_sample) or is_too_long(raw_sample) or has_only_single_utterance(raw_sample):\n",
    "        return\n",
    "\n",
    "    utterances = []\n",
    "    speakers = []\n",
    "    \n",
    "    for turn in raw_sample:\n",
    "        for sp, item in zip([user_id, system_id], ['user utterance', 'system response']):\n",
    "            ut = turn[item]\n",
    "            if ut == '':\n",
    "                continue\n",
    "            utterances.append(ut)\n",
    "            speakers.append(sp)\n",
    "    \n",
    "    if bound is not None and any(is_above_bound(ut, tokenizer, bound) for ut in utterances):\n",
    "        # if there're any utterances with exceeding length\n",
    "        return\n",
    "    \n",
    "    return utterances, speakers\n",
    "\n",
    "\n",
    "def is_empty(dia):\n",
    "    return len(dia) == 0\n",
    "\n",
    "\n",
    "def is_too_long(dia):\n",
    "    return len(dia) > 10\n",
    "\n",
    "\n",
    "def has_only_single_utterance(dia):\n",
    "    return len(dia) == 1 and (dia[0]['user utterance'] == '' or dia[0]['system response'] == '')\n",
    "\n",
    "\n",
    "def is_above_bound(ut, tokenizer, bound):\n",
    "    return len(tokenizer(ut)['input_ids']) > bound\n",
    "\n",
    "\n",
    "def get_record_iterator(name, tokenizer, bound):\n",
    "    dataset = load_dataset('Salesforce/dialogstudio', name)['train']['log']\n",
    "    for i, raw_dia in enumerate(dataset):\n",
    "        dia = preprocess_dialogue(raw_dia, tokenizer, bound)\n",
    "        if dia is None:\n",
    "            continue\n",
    "        utterances, speakers = dia\n",
    "        dia = Dialogue(\n",
    "            utterances=utterances,\n",
    "            speakers=speakers,\n",
    "            source_dataset_name=name,\n",
    "            idx_within_source=i,\n",
    "            # idx=None\n",
    "        )\n",
    "        dct = dia.asdict()\n",
    "        dct['content'] = json.dumps(dct['content'])\n",
    "        yield pa.RecordBatch.from_pylist([dct])\n",
    "\n",
    "\n",
    "def train_test_split(data: List[Dialogue], frac=0.9, seed=0):\n",
    "    \"\"\"resulting sizes:\n",
    "    - train: `frac`\n",
    "    - test: `(1 - frac) // 2`\n",
    "    - val: `(1 - frac) // 2`\"\"\"\n",
    "    \n",
    "    set_seet(seed)\n",
    "    shuffle(data)\n",
    "\n",
    "    # assign indices after shuffling the dataset\n",
    "    for i, dia in enumerate(data):\n",
    "        dia.idx = i\n",
    "\n",
    "    n_total = len(data)\n",
    "    train_size = ceil(frac * n_total)\n",
    "    test_size = (n_total - train_size) // 2\n",
    "    val_size = n_total - train_size - test_size\n",
    "\n",
    "    res = {\n",
    "        'train': data[:train_size],\n",
    "        'test': data[train_size:train_size+test_size],\n",
    "        'val': data[train_size+test_size:]\n",
    "    }\n",
    "\n",
    "    print('dataset splits sizes:')\n",
    "    print(f'{n_total=}, {train_size=}, {test_size=}, {val_size=}')\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def make_pairs(dialogues):\n",
    "    res = []\n",
    "    for dia in tqdm(dialogues, desc='making pairs'):\n",
    "        pairs = []\n",
    "        for i in range(len(dia)-1):\n",
    "            pairs.append((dia[:i+1], dia[i+1]))\n",
    "        res.extend(pairs)\n",
    "    shuffle(res)\n",
    "    res = [ContextResponsePair(context=c, response=r, idx=i) for i, (c, r) in enumerate(res)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "from mylib.utils.training import seed_everything\n",
    "import itertools as it\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "seed_everything(0)\n",
    "\n",
    "# supress warnings about long sequences\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "\n",
    "#! not the same as roberta, replace in future\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/mpnet-base')\n",
    "\n",
    "# load datasets from hugging face, parse, filter and merge into single list\n",
    "record_iterator_list = []\n",
    "for dataset_name in names:\n",
    "    iterator = get_record_iterator(dataset_name, tokenizer, upper_bounds[dataset_name])\n",
    "    record_iterator_list.append(iterator)\n",
    "\n",
    "chained_iterator = it.chain.from_iterable(record_iterator_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = iter(chained_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.RecordBatch\n",
       "content: string\n",
       "source_dataset_name: string\n",
       "idx_within_source: int64\n",
       "----\n",
       "content: [\"[{\"utterance\": \"I'd like 2 tickets to see Zoolander 2 tomorrow at Regal Meridian 16 theater in Seattle at 9:25 PM\", \"speaker\": 0}, {\"utterance\": \"Okay, your purchase of 2 tickets for Zoolander 2 is confirmed.\", \"speaker\": 1}]\"]\n",
       "source_dataset_name: [\"MS-DC\"]\n",
       "idx_within_source: [0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 3.00k/3.00k [00:00<00:00, 12.7MB/s]\n",
      "Downloading data: 100%|██████████| 15.6M/15.6M [00:05<00:00, 2.78MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:05<00:00,  5.60s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 311.64it/s]\n",
      "Generating train split: 15015 examples [00:00, 183594.95 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 15015\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('argilla/databricks-dolly-15k-curated-en')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 13513\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 1502\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test = dataset['train'].train_test_split(test_size=.1, shuffle=True, seed=0)\n",
    "train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 751\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 751\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_val = train_test['test'].train_test_split(test_size=.5, shuffle=True, seed=0)\n",
    "test_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 751\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 751\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_val['val'] = test_val['train']\n",
    "del test_val['train']\n",
    "test_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 13513\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 751\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 751\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "res_dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': test_val['test'],\n",
    "    'val': test_val['val']\n",
    "})\n",
    "res_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (10/10 shards): 100%|██████████| 13513/13513 [00:00<00:00, 32162.12 examples/s]\n",
      "Saving the dataset (5/5 shards): 100%|██████████| 751/751 [00:00<00:00, 14672.57 examples/s]\n",
      "Saving the dataset (5/5 shards): 100%|██████████| 751/751 [00:00<00:00, 14196.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "res_dataset.save_to_disk('data-2/testing_hf', num_shards={'train': 10, 'test': 5, 'val': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alekseev_ilya/dialogue-augmentation/VENV/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 13513\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 751\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n",
       "        num_rows: 751\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('data-2/testing_hf')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '12571',\n",
       " 'category': 'information_extraction',\n",
       " 'original-instruction': 'Where did the random walk hypothesis come from?',\n",
       " 'original-context': 'The random walk hypothesis is a financial theory stating that stock market prices evolve according to a random walk (so price changes are random) and thus cannot be predicted.\\nThe concept can be traced to French broker Jules Regnault who published a book in 1863, and then to French mathematician Louis Bachelier whose Ph.D. dissertation titled \"The Theory of Speculation\" (1900) included some remarkable insights and commentary. The same ideas were later developed by MIT Sloan School of Management professor Paul Cootner in his 1964 book The Random Character of Stock Market Prices.[1] The term was popularized by the 1973 book A Random Walk Down Wall Street by Burton Malkiel, a professor of economics at Princeton University,[2] and was used earlier in Eugene Fama\\'s 1965 article \"Random Walks In Stock Market Prices\",[3] which was a less technical version of his Ph.D. thesis. The theory that stock prices move randomly was earlier proposed by Maurice Kendall in his 1953 paper, The Analysis of Economic Time Series, Part 1: Prices.[4]',\n",
       " 'original-response': 'It is said that the random walk hypothesis originated back in the mid 1800s from a French financier named Jules Regnault.  The random walk hypothesis notes that stock price movements are fully random and cannot be easily forecast.',\n",
       " 'new-instruction': {'user_id': [None],\n",
       "  'value': ['Where did the random walk hypothesis come from?'],\n",
       "  'status': ['submitted']},\n",
       " 'new-context': {'user_id': [None],\n",
       "  'value': ['The random walk hypothesis is a financial theory stating that stock market prices evolve according to a random walk (so price changes are random) and thus cannot be predicted.\\nThe concept can be traced to French broker Jules Regnault who published a book in 1863, and then to French mathematician Louis Bachelier whose Ph.D. dissertation titled \"The Theory of Speculation\" (1900) included some remarkable insights and commentary. The same ideas were later developed by MIT Sloan School of Management professor Paul Cootner in his 1964 book The Random Character of Stock Market Prices. The term was popularized by the 1973 book A Random Walk Down Wall Street by Burton Malkiel, a professor of economics at Princeton University, and was used earlier in Eugene Fama\\'s 1965 article \"Random Walks In Stock Market Prices\", which was a less technical version of his Ph.D. thesis. The theory that stock prices move randomly was earlier proposed by Maurice Kendall in his 1953 paper, The Analysis of Economic Time Series, Part 1: Prices.'],\n",
       "  'status': ['submitted']},\n",
       " 'new-response': {'user_id': [None],\n",
       "  'value': ['It is said that the random walk hypothesis originated back in the mid 1800s from a French financier named Jules Regnault.  The random walk hypothesis notes that stock price movements are fully random and cannot be easily forecast.'],\n",
       "  'status': ['submitted']},\n",
       " 'external_id': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '12083',\n",
       " 'category': 'classification',\n",
       " 'original-instruction': 'Tell me whether these sports require a ball or not to play: basketball, hockey, track and field, soccer, surfing, water polo, karate, football, gymnastics, skateboarding, golf, tennis, boxing, baseball, rugby, weightlifting, softball, rowing',\n",
       " 'original-context': '',\n",
       " 'original-response': 'Basketball, soccer, water polo, football, golf, tennis, baseball, rugby, and softball require a ball to play. Hockey, track and field, surfing, karate, gymnastics, skateboarding, boxing, weightlifting, and rowing do not require a ball.',\n",
       " 'new-instruction': {'user_id': [None],\n",
       "  'value': ['Tell me whether these sports require a ball or not to play: basketball, hockey, track and field, soccer, surfing, water polo, karate, football, gymnastics, skateboarding, golf, tennis, boxing, baseball, rugby, weightlifting, softball, rowing'],\n",
       "  'status': ['submitted']},\n",
       " 'new-context': {'user_id': [None], 'value': [''], 'status': ['submitted']},\n",
       " 'new-response': {'user_id': [None],\n",
       "  'value': ['Basketball, soccer, water polo, football, golf, tennis, baseball, rugby, and softball require a ball to play. Hockey, track and field, surfing, karate, gymnastics, skateboarding, boxing, weightlifting, and rowing do not require a ball.'],\n",
       "  'status': ['submitted']},\n",
       " 'external_id': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1479',\n",
       " 'category': 'information_extraction',\n",
       " 'original-instruction': 'Which version of the church existed in 1809?',\n",
       " 'original-context': 'The area was settled by Canary Islanders and Acadians from 1779 on and the parish was formed in 1793 with its first church being \"little more than a shack\". It was replaced by a more permanent one in 1819, which in turn was replaced by this one in 1856, quite close to the site of the 1793 church. The parish\\'s governing \"Fabrique\", or council of wardens, reportedly appointed a committee to measure the Catholic church in Thibodaux, Louisiana, and',\n",
       " 'original-response': 'The original version of the church still existed in 1809, it was 10 years later that a more permanent one replaced it.',\n",
       " 'new-instruction': {'user_id': [None],\n",
       "  'value': ['Which version of the church existed in 1809?'],\n",
       "  'status': ['submitted']},\n",
       " 'new-context': {'user_id': [None],\n",
       "  'value': ['The area was settled by Canary Islanders and Acadians from 1779 on and the parish was formed in 1793 with its first church being \"little more than a shack\". It was replaced by a more permanent one in 1819, which in turn was replaced by this one in 1856, quite close to the site of the 1793 church. The parish\\'s governing \"Fabrique\", or council of wardens, reportedly appointed a committee to measure the Catholic church in Thibodaux, Louisiana, and'],\n",
       "  'status': ['submitted']},\n",
       " 'new-response': {'user_id': [None],\n",
       "  'value': ['The original version of the church still existed in 1809, it was 10 years later that a more permanent one replaced it.'],\n",
       "  'status': ['submitted']},\n",
       " 'external_id': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['val'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alekseev_ilya/dialogue-augmentation/VENV/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['content', 'source_dataset_name', 'idx_within_source', 'context', 'response', 'split_index'],\n",
       "        num_rows: 450750\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['content', 'source_dataset_name', 'idx_within_source', 'context', 'response', 'split_index'],\n",
       "        num_rows: 25042\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['content', 'source_dataset_name', 'idx_within_source', 'context', 'response', 'split_index'],\n",
       "        num_rows: 25042\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('data-2/train/context-response-pairs/')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '[{\"utterance\": \"Hi.\", \"speaker\": 0}, {\"utterance\": \"Hello.\", \"speaker\": 1}, {\"utterance\": \"My name is Timothy Lewis.\", \"speaker\": 0}, {\"utterance\": \"How can I help you?\", \"speaker\": 1}, {\"utterance\": \"I need to book a flight ticket from JFK to PHX. Please help me in booking a flight ticket.\", \"speaker\": 0}, {\"utterance\": \"Can I know your travelling dates?\", \"speaker\": 1}, {\"utterance\": \"My journey dates are from June, 23 to June, 25.\", \"speaker\": 0}, {\"utterance\": \"Do you have any preference?\", \"speaker\": 1}, {\"utterance\": \"I need to depart in the afternoon and I need a connecting flight.\", \"speaker\": 0}, {\"utterance\": \"Sorry, no flights found for your travelling dates.\", \"speaker\": 1}, {\"utterance\": \"No problem, thanks for your information.\", \"speaker\": 0}, {\"utterance\": \"Thank you.\", \"speaker\": 1}]',\n",
       " 'source_dataset_name': 'AirDialogue',\n",
       " 'idx_within_source': 217470,\n",
       " 'context': [[{'speaker': 0,\n",
       "    'utterance': 'I need to book a flight ticket from JFK to PHX. Please help me in booking a flight ticket.'},\n",
       "   {'speaker': 1, 'utterance': 'Can I know your travelling dates?'},\n",
       "   {'speaker': 0,\n",
       "    'utterance': 'My journey dates are from June, 23 to June, 25.'}],\n",
       "  [{'speaker': 0,\n",
       "    'utterance': 'I need to book a flight ticket from JFK to PHX. Please help me in booking a flight ticket.'},\n",
       "   {'speaker': 1, 'utterance': 'Can I know your travelling dates?'},\n",
       "   {'speaker': 0,\n",
       "    'utterance': 'My journey dates are from June, 23 to June, 25.'}]],\n",
       " 'response': [[{'speaker': 1, 'utterance': 'Do you have any preference?'},\n",
       "   {'speaker': 0,\n",
       "    'utterance': 'I need to depart in the afternoon and I need a connecting flight.'},\n",
       "   {'speaker': 1,\n",
       "    'utterance': 'Sorry, no flights found for your travelling dates.'}],\n",
       "  [{'speaker': 1, 'utterance': 'Do you have any preference?'},\n",
       "   {'speaker': 0,\n",
       "    'utterance': 'I need to depart in the afternoon and I need a connecting flight.'},\n",
       "   {'speaker': 1,\n",
       "    'utterance': 'Sorry, no flights found for your travelling dates.'}]],\n",
       " 'split_index': [7, 7]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['val'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alekseev_ilya/dialogue-augmentation/VENV/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/alekseev_ilya/dialogue-augmentation/VENV/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('data-2/source/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450750"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mylib.datasets import DynamicContextResponseDataset\n",
    "\n",
    "dataset = DynamicContextResponseDataset('data-2/source', split='train', context_size=10)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [{'speaker': 0, 'utterance': 'Hi. I am Dorothy Anderson.'},\n",
       "  {'speaker': 1, 'utterance': 'Hello, how can I assist you today?'},\n",
       "  {'speaker': 0,\n",
       "   'utterance': 'I want to cancel my reservation as my lab exam got cancelled. Can you help me with cancellation?'},\n",
       "  {'speaker': 1, 'utterance': 'Sure, please wait for a moment.'},\n",
       "  {'speaker': 0, 'utterance': 'Ok.'},\n",
       "  {'speaker': 1, 'utterance': 'Sorry, no reservation found with your name.'}],\n",
       " 'target': [{'speaker': 0, 'utterance': \"It's Ok. Thanks for your response.\"},\n",
       "  {'speaker': 1, 'utterance': 'Thank you for choosing us.'}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Y0CQ6',\n",
       " '5ZT4W',\n",
       " 'N6ISI',\n",
       " 'GQ8JT',\n",
       " 'GEV49',\n",
       " 'GW1UN',\n",
       " '9427Q',\n",
       " 'D9AFZ',\n",
       " 'A5VPU',\n",
       " 'EMOPJ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "def generate_id(length):\n",
    "    alphabet = string.ascii_uppercase + string.digits\n",
    "    return ''.join(random.choice(alphabet) for _ in range(length))\n",
    "\n",
    "[generate_id(5) for _ in range(10)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
